{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2rVpUrpAyAG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6xW9fIIaxs6"
      },
      "outputs": [],
      "source": [
        "def logistic():\n",
        "    from sklearn.linear_model import LogisticRegressionCV\n",
        "    model = LogisticRegressionCV()\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def knn():\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    model = KNeighborsClassifier(n_neighbors=7)\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def naive_bayes():\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    model = GaussianNB()\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def decisionTree():\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    model = DecisionTreeClassifier(max_depth=500)\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def randomForest():\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=500)\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "def svm():\n",
        "    from sklearn.svm import SVC\n",
        "    model = SVC(C=2, kernel='rbf')\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "def catboost():\n",
        "    from catboost import CatBoostClassifier\n",
        "    model = CatBoostClassifier(verbose=False)\n",
        "    model.fit(x_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def xgboost():\n",
        "    from xgboost import XGBClassifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(x_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0vsNHon7Qy0"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ph-qRzQ7Rm-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "def evaluate_model_train(model):\n",
        "    y_true = y_train\n",
        "    y_pred = model.predict(x_train)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def evaluate_model_test(model):\n",
        "    y_true = y_test\n",
        "    y_pred = model.predict(x_test)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJSBFrz7h1b"
      },
      "source": [
        "## Evaluation Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyNw5KcG7ZdO"
      },
      "outputs": [],
      "source": [
        "#list of model functions\n",
        "model_functions = [\n",
        "    {\"name\": \"Logistic Regression\", \"function\": logistic},\n",
        "    {\"name\": \"K Nearest Neighbour\", \"function\": knn},\n",
        "    {\"name\": \"Naive Bayes\", \"function\": naive_bayes},\n",
        "    {\"name\": \"Decision Tree\", \"function\": decisionTree},\n",
        "    {\"name\": \"Random Forest\", \"function\": randomForest},\n",
        "    {\"name\": \"SVM\", \"function\": svm},\n",
        "    {\"name\": \"CatBoost\", \"function\": catboost},\n",
        "    {\"name\": \"XGBoost\", \"function\": xgboost}\n",
        "]\n",
        "\n",
        "# empty list to collect the modelwise report\n",
        "model_evaluation_report = []\n",
        "\n",
        "# iterate over the list, create model and evaluate the model\n",
        "for model_info in model_functions:\n",
        "    model = model_info[\"function\"]()\n",
        "    metrics_train = evaluate_model_train(model)\n",
        "    metrics_test = evaluate_model_test(model)\n",
        "    model_evaluation_report.append({\n",
        "        \"name\": model_info[\"name\"],\n",
        "        \"train_accuracy\": metrics_train[0],\n",
        "        \"train_precision\": metrics_train[1],\n",
        "        \"train_recall\": metrics_train[2],\n",
        "        \"train_f1\": metrics_train[3],\n",
        "        \"accuracy\": metrics_test[0],\n",
        "        \"precision\": metrics_test[1],\n",
        "        \"recall\": metrics_test[2],\n",
        "        \"f1\": metrics_test[3]\n",
        "\n",
        "    })\n",
        "\n",
        "# create a data frame of the result\n",
        "df_result = pd.DataFrame(model_evaluation_report)\n",
        "df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuIQq5vL9Epj"
      },
      "source": [
        "## Save model (Based on evaluation report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljQSR8dE9Gb5"
      },
      "outputs": [],
      "source": [
        "# save_model(model, 'model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbl9UU8q9EjK"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
